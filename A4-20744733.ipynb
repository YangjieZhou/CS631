{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS431/631 Big Data Infrastructure\n",
    "### Winter 2018 - Assignment 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please edit this (text) cell to provide your name and UW student ID number!**\n",
    "* **Name:** _Yangjie Zhou_\n",
    "* **ID:** _20744733_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Overview\n",
    "For this assignment, you will be using Python and Spark to perform some simple analyses on relational (tabular) data.  You will use Spark to read tabular data from files and then answer simple queries about the data in those tables.\n",
    "\n",
    "In addition to Python and Spark, you will need to use a little bit of SQL.  If you are already familiar with SQL, great.  If not, you will want to spend a short amount of time getting familiar with the basics.   Type \"SQL tutorial\" into your favorite search engine, and you will find many examples of text, interactive and video tutorials.   As a simple starting point, you might also want to look at [these slides](https://cs.uwaterloo.ca/~kmsalem/courses/cs743/F14/slides/sql.pdf), which give a short introduction to SQL.   Even this is much more than you will need for this assignment.\n",
    "\n",
    "You will be working with tabular data based on the schema for the TPC-H benchmark, which is a standard test used to measure the performance of relational database systems.   The schema defines the tables that exist in the database, the information in each table, and relationships between information in one table and information in another.   The TPC-H schema models business information:  customers, orders, parts, suppliers, and so on.   You can find a diagram illustrating the TPC-H schema in the lecture notes.   You can also find it on page 13 of the [TPC-H benchmark specification](http://www.tpc.org/TPC_Documents_Current_Versions/pdf/tpc-h_v2.17.3.pdf).   The TPC-H schema is important for this assignment, so make sure that you keep this schema handy.\n",
    "\n",
    "Finally, for this assignment you will be using Spark in a slightly different way than you have used it so far.  For previous assignments, you have used the original Spark RDD interface.   For this assignment, you will be using the newer Spark interface, which is based on DataFrames.   DataFrames are RDDs in which each element is constrained to have the same structure.   You can think of a DataFrame like a table in a relational database.   Each element of the DataFrame is a row or record in the table.   All records have the same structure.   There is a programming guide for Spark DataFrames [here](https://spark.apache.org/docs/latest/sql-programming-guide.html).  Start with that. There is also a [more detailed guide to the full programming interface](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html).\n",
    "\n",
    "---\n",
    "#### Getting Started\n",
    "To get started, let's initialize Spark, load a couple of the TPC-H tables, and run some simple queries.\n",
    "First, as always, we need to tell the Python interpreter where to find Spark, so run `findspark.init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/u/cs451/packages/spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we launch Spark.  When you used the RDD interface for previous assignments, you created a `SparkContext` when you launched Spark.   To use Spark SQL and the DataFrame interface, you instead create a `SparkSession`.   You do that as shown in the next cell (run it!).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"YourTest\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create DataFrames from the TPC-H data files.  We have installed the TPC-H data files in the directory `/u/cs451/data/TPC-H-0.1-TXT/`.   There is one file for each table in the TPC-H database, e.g., `nation.tbl` for the TPC-H Nation table, `customer.tbl` for the TPC-H Customer table, and so on.    These are plain text csv files, with the character | used as a field separator.\n",
    "\n",
    "Create a Spark DataFrame corresponding to the TPC-H Nation table by loading the data from the `nation.tbl` file.   Run the code in the next cell to do this.   After you run this code, `nation_raw` will refer to your new Spark DataFrame.   The Spark `show()` method will display a small (default 20) number of elements from the DataFrame, so that you can inspect them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+--------------------+----+\n",
      "|_c0|       _c1|_c2|                 _c3| _c4|\n",
      "+---+----------+---+--------------------+----+\n",
      "|  0|   ALGERIA|  0| haggle. carefull...|null|\n",
      "|  1| ARGENTINA|  1|al foxes promise ...|null|\n",
      "|  2|    BRAZIL|  1|y alongside of th...|null|\n",
      "|  3|    CANADA|  1|eas hang ironic, ...|null|\n",
      "|  4|     EGYPT|  4|y above the caref...|null|\n",
      "|  5|  ETHIOPIA|  0|ven packages wake...|null|\n",
      "|  6|    FRANCE|  3|refully final req...|null|\n",
      "|  7|   GERMANY|  3|l platelets. regu...|null|\n",
      "|  8|     INDIA|  2|ss excuses cajole...|null|\n",
      "|  9| INDONESIA|  2| slyly express as...|null|\n",
      "| 10|      IRAN|  4|efully alongside ...|null|\n",
      "| 11|      IRAQ|  4|nic deposits boos...|null|\n",
      "| 12|     JAPAN|  2|ously. final, exp...|null|\n",
      "| 13|    JORDAN|  4|ic deposits are b...|null|\n",
      "| 14|     KENYA|  0| pending excuses ...|null|\n",
      "| 15|   MOROCCO|  0|rns. blithely bol...|null|\n",
      "| 16|MOZAMBIQUE|  0|s. ironic, unusua...|null|\n",
      "| 17|      PERU|  1|platelets. blithe...|null|\n",
      "| 18|     CHINA|  2|c dependencies. f...|null|\n",
      "| 19|   ROMANIA|  3|ular asymptotes a...|null|\n",
      "+---+----------+---+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nation_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/nation.tbl\",sep='|',inferSchema=True)\n",
    "nation_raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a DataFrame to work with.   The columns of the DataFrame correspond to the fields of the TPC-H Nation table, so have a look at the TPC-H schema diagram to see what you are dealing with.   Column c0 is the NATIONKEY, column c1 is the NAME, c2 is the REGIONKEY, and so on.   Since this is a synthetic database, you'll notice that the data in some of the fields (like the COMMENT field) consists of random words.   That's fine.   You can also ask Spark to tell you about the type of data in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'int'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, let's clean this DataFrame up a bit, to make it easier to use.   First, let's assign names to the columns, so that we can remember what information each column holds.   Second, you'll notice that Spark has created an extra final column (filled with `null` values) because each line in the input file ends with a separator character (|).  Let's drop that final column, since we don't need it.   Run the following code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+--------------------+\n",
      "|NationKey|      Name|RegionKey|             Comment|\n",
      "+---------+----------+---------+--------------------+\n",
      "|        0|   ALGERIA|        0| haggle. carefull...|\n",
      "|        1| ARGENTINA|        1|al foxes promise ...|\n",
      "|        2|    BRAZIL|        1|y alongside of th...|\n",
      "|        3|    CANADA|        1|eas hang ironic, ...|\n",
      "|        4|     EGYPT|        4|y above the caref...|\n",
      "|        5|  ETHIOPIA|        0|ven packages wake...|\n",
      "|        6|    FRANCE|        3|refully final req...|\n",
      "|        7|   GERMANY|        3|l platelets. regu...|\n",
      "|        8|     INDIA|        2|ss excuses cajole...|\n",
      "|        9| INDONESIA|        2| slyly express as...|\n",
      "|       10|      IRAN|        4|efully alongside ...|\n",
      "|       11|      IRAQ|        4|nic deposits boos...|\n",
      "|       12|     JAPAN|        2|ously. final, exp...|\n",
      "|       13|    JORDAN|        4|ic deposits are b...|\n",
      "|       14|     KENYA|        0| pending excuses ...|\n",
      "|       15|   MOROCCO|        0|rns. blithely bol...|\n",
      "|       16|MOZAMBIQUE|        0|s. ironic, unusua...|\n",
      "|       17|      PERU|        1|platelets. blithe...|\n",
      "|       18|     CHINA|        2|c dependencies. f...|\n",
      "|       19|   ROMANIA|        3|ular asymptotes a...|\n",
      "+---------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nation = nation_raw.toDF('NationKey','Name','RegionKey','Comment','extra').drop('extra').cache()\n",
    "nation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This style of code should look familar to you.  We started with the `nation_raw` DataFrame and applied a series of DataFrame operations (`toDF`, `drop`, and `cache`).   This is just like the RDD interface, except now we are applying DataFrame operations to DataFrames, instead of RDD operations to RDDs.\n",
    "\n",
    "Next, let's load up the TPC-H Supplier table, and then try performing some queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
      "|SuppKey|              Name|             Address|NationKey|          Phone|AcctBal|             Comment|\n",
      "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
      "|      1|Supplier#000000001| N kD4on9OM Ipw3,...|       17|27-918-335-1736|5755.94|each slyly above ...|\n",
      "|      2|Supplier#000000002|89eJ5ksX3ImxJQBvx...|        5|15-679-861-2259|4032.68| slyly bold instr...|\n",
      "|      3|Supplier#000000003|q1,G3Pj6OjIuUYfUo...|        1|11-383-516-1199| 4192.4|blithely silent r...|\n",
      "|      4|Supplier#000000004|Bk7ah4CK8SYQTepEm...|       15|25-843-787-7479|4641.08|riously even requ...|\n",
      "|      5|Supplier#000000005|   Gcdm2rJRzl5qlTVzc|       11|21-151-690-3663|-283.84|. slyly regular p...|\n",
      "|      6|Supplier#000000006|        tQxuVm7s7CnK|       14|24-696-997-4969|1365.79|final accounts. r...|\n",
      "|      7|Supplier#000000007|s,4TicNGB4uO6PaSq...|       23|33-990-965-2201|6820.35|s unwind silently...|\n",
      "|      8|Supplier#000000008|9Sq4bBH2FQEmaFOoc...|       17|27-498-742-3860|7627.85|al pinto beans. a...|\n",
      "|      9|Supplier#000000009|1KhUgZegwM3ua7dsY...|       10|20-403-398-8662|5302.37|s. unusual, even ...|\n",
      "|     10|Supplier#000000010|  Saygah3gYWMp72i PY|       24|34-852-489-8585|3891.91|ing waters. regul...|\n",
      "|     11|Supplier#000000011|    JfwTs,LZrV, M,9C|       18|28-613-996-1505|3393.08|y ironic packages...|\n",
      "|     12|Supplier#000000012|         aLIW  q0HYd|        8|18-179-925-7181|1432.69|al packages nag a...|\n",
      "|     13|Supplier#000000013|HK71HQyWoqRWOX8GI...|        3|13-727-620-7813|9107.22|requests engage r...|\n",
      "|     14|Supplier#000000014|     EXsnO5pTNj4iZRm|       15|25-656-247-5058|9189.82|l accounts boost....|\n",
      "|     15|Supplier#000000015|olXVbNBfVzRqgokr1...|        8|18-453-357-6394| 308.56| across the furio...|\n",
      "|     16|Supplier#000000016|YjP5C55zHDXL7LalK...|       22|32-822-502-4215|2972.26|ously express ide...|\n",
      "|     17|Supplier#000000017|c2d,ESHRSkK3WYnxp...|       19|29-601-884-9219|1687.81|eep against the f...|\n",
      "|     18|Supplier#000000018|    PGGVE5PWAMwKDZw |       16|26-729-551-1115|7040.82|accounts snooze s...|\n",
      "|     19|Supplier#000000019|edZT3es,nBFD8lBXT...|       24|34-278-310-2731|6150.38|refully final fox...|\n",
      "|     20|Supplier#000000020|iybAE,RmTymrZVYaF...|        3|13-715-945-6730| 530.82|n, ironic ideas w...|\n",
      "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supplier_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/supplier.tbl\",sep='|',inferSchema=True).drop(\"_c7\")\n",
    "supplier = supplier_raw.toDF(\"SuppKey\",\"Name\",\"Address\",\"NationKey\",\"Phone\",\"AcctBal\",\"Comment\").cache()\n",
    "supplier.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Writing Queries\n",
    "There are two equivalent ways of writing queries over Spark DataFrames.   The first way is to assign a \"view name\" to the DataFrame, and then write SQL queries referring to those view names using the `sql` operation.  \n",
    "\n",
    "The code below gives the view names \"nation\" and \"supplier\" to the two DataFrames we've already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier.createOrReplaceTempView(\"supplier\")\n",
    "nation.createOrReplaceTempView(\"nation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write SQL queries that refer to the \"supplier\" and \"nation\" views as tables.   For example, suppose we want to see the names and addresses of suppliers who have account balances above 9900.00:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------+\n",
      "|              Name|             Address|AcctBal|\n",
      "+------------------+--------------------+-------+\n",
      "|Supplier#000000049|     Nvq 6macF4GtJvz|9915.24|\n",
      "|Supplier#000000234|iMrk7HUD87at3IIh4rBi| 9957.0|\n",
      "|Supplier#000000693|S,mnHfsroFOVieQGd...|9956.55|\n",
      "|Supplier#000000855|ekQwhb9fh5VGIvMBJ...|9964.88|\n",
      "|Supplier#000000892|j6prA4M3sX9a9xHem...|9993.46|\n",
      "+------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_result = spark.sql(\"select Name, Address, AcctBal from supplier where AcctBal > 9900.00\")\n",
    "q1_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the `sql` command runs the SQL query against the supplier table.   It returns the query result as a new DataFrame, which `q1_result` refers to.\n",
    "\n",
    "Instead of writing your queries in SQL and running them using `sql`, it is possible to do the same thing by applying a sequence of DataFrame operations to the input DataFrames, as you did when you were using the RDD interface in the previous assignments.    For example, to answer the same query that we just answered using SQL, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------+\n",
      "|              Name|             Address|AcctBal|\n",
      "+------------------+--------------------+-------+\n",
      "|Supplier#000000049|     Nvq 6macF4GtJvz|9915.24|\n",
      "|Supplier#000000234|iMrk7HUD87at3IIh4rBi| 9957.0|\n",
      "|Supplier#000000693|S,mnHfsroFOVieQGd...|9956.55|\n",
      "|Supplier#000000855|ekQwhb9fh5VGIvMBJ...|9964.88|\n",
      "|Supplier#000000892|j6prA4M3sX9a9xHem...|9993.46|\n",
      "+------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_resultB = supplier.filter(\"AcctBal > 9900.00\").select('Name','Address','AcctBal')\n",
    "q1_resultB.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods should give the same result.   Internally, Spark handles both similarly.   For this assignment, you'll be asked to try out both methods.\n",
    "\n",
    "Now it is time for you to try writing your own queries.\n",
    "\n",
    "---\n",
    "#### Question 1 (2/25 marks)\n",
    "In the cell below, write a query that will return the ORDERKEY, ORDERDATE, and TOTALPRICE of the five orders with the highest TOTALPRICE.   Express your query in SQL, and use `sql` to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|OrderKey|           OrderDate|TotalPrice|\n",
      "+--------+--------------------+----------+\n",
      "|  279812|1994-02-19 00:00:...| 479129.21|\n",
      "|  370726|1996-09-29 00:00:...|  460099.4|\n",
      "|   66659|1993-10-15 00:00:...| 458396.42|\n",
      "|  253639|1998-01-23 00:00:...| 456532.89|\n",
      "|  502886|1994-04-12 00:00:...| 456423.88|\n",
      "+--------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 1 here\n",
    "orders_raw=spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/orders.tbl\",sep=\"|\",inferSchema=True)\n",
    "orders = orders_raw.toDF(\"OrderKey\",\"CustKey\",\"OrderStatus\",\n",
    "                         \"TotalPrice\",\"OrderDate\",\"Order-Priority\",\n",
    "                         \"Clerk\",\"Ship-Priority\",\"Comment\",\"extra\").drop(\"extra\").cache()\n",
    "orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "q1_result = spark.sql(\"SELECT OrderKey, OrderDate, TotalPrice FROM orders ORDER BY TotalPrice DESC LIMIT 5\")\n",
    "q1_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 2 (2/25 marks)\n",
    "In the cell below, answer the same query you answered in Question 1, but this time do not use the `sql` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|OrderKey|           OrderDate|TotalPrice|\n",
      "+--------+--------------------+----------+\n",
      "|  279812|1994-02-19 00:00:...| 479129.21|\n",
      "|  370726|1996-09-29 00:00:...|  460099.4|\n",
      "|   66659|1993-10-15 00:00:...| 458396.42|\n",
      "|  253639|1998-01-23 00:00:...| 456532.89|\n",
      "|  502886|1994-04-12 00:00:...| 456423.88|\n",
      "+--------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 2 here\n",
    "q2_result=orders.select(\"OrderKey\", \"OrderDate\", \"TotalPrice\")\\\n",
    "                .orderBy(\"TotalPrice\",ascending=False)\\\n",
    "                .limit(5)\n",
    "q2_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 3 (3/25 marks)\n",
    "In the cell below, write code that will prompt for a Customer key, and then return the customer's name as well as the ORDERDATE and TOTALPRICE of that customer's most recent order.   Express the query in SQL, and use `sql` to execute it.   You will need to use information from multiple tables to generate your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input the customer key(press Enter to quit):1\n",
      "+------------------+--------------------+----------+\n",
      "|              Name|           OrderDate|TotalPrice|\n",
      "+------------------+--------------------+----------+\n",
      "|Customer#000000001|1997-03-04 00:00:...| 268835.44|\n",
      "+------------------+--------------------+----------+\n",
      "\n",
      "input the customer key(press Enter to quit):2\n",
      "+------------------+--------------------+----------+\n",
      "|              Name|           OrderDate|TotalPrice|\n",
      "+------------------+--------------------+----------+\n",
      "|Customer#000000002|1998-05-22 00:00:...| 122500.55|\n",
      "+------------------+--------------------+----------+\n",
      "\n",
      "input the customer key(press Enter to quit):3\n",
      "+----+---------+----------+\n",
      "|Name|OrderDate|TotalPrice|\n",
      "+----+---------+----------+\n",
      "+----+---------+----------+\n",
      "\n",
      "input the customer key(press Enter to quit):\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 3 here\n",
    "# 用户名字（在用户table中）\n",
    "# 订单时间和总价（在订单table中，更具体的应该在排序过的用户的订单中）\n",
    "customer_raw=spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/customer.tbl\",sep=\"|\",inferSchema=True)\n",
    "customer = customer_raw.toDF(\"CustKey\",\"Name\",\"Address\",\"NationKey\",\"Phone\",\n",
    "                             \"Acctbal\",\"Mktsegment\",\"Comment\",\"extra\").drop(\"extra\").cache()\n",
    "customer.createOrReplaceTempView(\"customer\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        customer_key=input(\"input the customer key(press Enter to quit):\")\n",
    "        if len(customer_key)==0:\n",
    "            break\n",
    "        customer_key=int(customer_key)\n",
    "        if customer_key<=0 or customer_key>15000:\n",
    "            continue\n",
    "    except  ValueError:\n",
    "        print(\"customer key must be a positive integer ranging from 1 to 15000!\")\n",
    "        continue\n",
    "    ## using the subquery to speed up\n",
    "    q3_result=spark.sql(\"SELECT c.Name,o.OrderDate,o.TotalPrice\\\n",
    "                        FROM orders o INNER JOIN (select CustKey,Name from customer where CustKey={0}) c \\\n",
    "                        ON c.CustKey=o.CustKey \\\n",
    "                        ORDER BY OrderDate DESC\\\n",
    "                        LIMIT 1\".format(customer_key))\n",
    "    q3_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 4 (3/25 marks)\n",
    "In the cell below, answer the same query you answered in Question 3, but this time do not use the `sql` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input the customer key(Press Enter to quit):1\n",
      "Row(Name='Customer#000000001', OrderDate=datetime.datetime(1997, 3, 4, 0, 0), TotalPrice=268835.44)\n",
      "input the customer key(Press Enter to quit):2\n",
      "Row(Name='Customer#000000002', OrderDate=datetime.datetime(1998, 5, 22, 0, 0), TotalPrice=122500.55)\n",
      "input the customer key(Press Enter to quit):3\n",
      "This customer never have an order\n",
      "input the customer key(Press Enter to quit):\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 4 here\n",
    "while True:\n",
    "    try:\n",
    "        customer_key=input(\"input the customer key(Press Enter to quit):\")\n",
    "        if len(customer_key)==0:\n",
    "            break\n",
    "        customer_key=int(customer_key)\n",
    "        if customer_key<=0 or customer_key>15000:\n",
    "            continue\n",
    "    except  ValueError:\n",
    "        print(\"Customer key must be a positive integer ranging from 1 to 15000!\")\n",
    "        continue\n",
    "    \n",
    "    ## judge whether this customer have ever ordered\n",
    "    if len(orders.filter(orders.CustKey==customer_key).collect())==0:\n",
    "        print(\"This customer never have an order\")\n",
    "        continue\n",
    "        \n",
    "    orders_short=orders.select(\"CustKey\",\"OrderDate\",\"TotalPrice\")\n",
    "    q4_result=customer.select(\"CustKey\",\"Name\")\\\n",
    "                    .where(customer.CustKey==customer_key)\\\n",
    "                    .join(orders_short,'CustKey')\\\n",
    "                    .orderBy(\"OrderDate\",ascending=False)\\\n",
    "                    .limit(1).drop('CustKey')\\\n",
    "                    .collect()\n",
    "    print(q4_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 5 (5/25 marks)\n",
    "In the cell below, write code that will prompt for a Nation name.  (Assume that nation names are unique in the Nation table.)   Report the number of distinct parts supplied by suppliers that are located in the given nation.\n",
    "\n",
    "You may answer this question with or without using `sql` - whichever you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input the Nation name(Press Enter to quite):INDIA\n",
      "+----------------+\n",
      "|count(NationKey)|\n",
      "+----------------+\n",
      "|              47|\n",
      "+----------------+\n",
      "\n",
      "input the Nation name(Press Enter to quite):CHINA\n",
      "+----------------+\n",
      "|count(NationKey)|\n",
      "+----------------+\n",
      "|              53|\n",
      "+----------------+\n",
      "\n",
      "input the Nation name(Press Enter to quite):\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 5 here\n",
    "nation_raw=spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/nation.tbl\",sep=\"|\",inferSchema=True)\n",
    "nation = nation_raw.toDF(\"NationKey\",\"Name\",\"RegionKey\",\"Comment\",\"extra\").drop(\"extra\").cache()\n",
    "nation.createOrReplaceTempView(\"nation\")\n",
    "\n",
    "while True:\n",
    "    nation_name=input(\"input the Nation name(Press Enter to quite):\")\n",
    "    if len(nation_name)==0:\n",
    "        break\n",
    "    q5_result=spark.sql(\"select count(s.NationKey) \\\n",
    "                        from supplier s\\\n",
    "                        where s.NationKey \\\n",
    "                        in (select NationKey from nation where nation.Name='{0}')\".format(nation_name))\n",
    "    q5_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 6 (5/25 marks)\n",
    "In the cell below, write code that will prompt for a BRAND, like those that appear in the Parts table.  Given the BRAND,\n",
    "report, for each nation, a count of that nation's suppliers of parts having that brand.   Your output should be a table of nations and their supplier counts. Each supplier should be counted at most once in a nation's total, even if that supplier produces multiple parts with the given brand. \n",
    "\n",
    "You may answer this question with or without using `sql` - whichever you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input the Brand(like \"Brand#13\" Press Enter to quite):Brand#13\n",
      "Country \t Count\n",
      "JAPAN \t 115\n",
      "RUSSIA \t 161\n",
      "ARGENTINA \t 115\n",
      "JORDAN \t 91\n",
      "FRANCE \t 118\n",
      "MOZAMBIQUE \t 108\n",
      "CANADA \t 129\n",
      "SAUDI ARABIA \t 159\n",
      "ETHIOPIA \t 117\n",
      "ROMANIA \t 112\n",
      "MOROCCO \t 120\n",
      "PERU \t 135\n",
      "INDONESIA \t 145\n",
      "EGYPT \t 127\n",
      "INDIA \t 141\n",
      "UNITED KINGDOM \t 126\n",
      "GERMANY \t 160\n",
      "IRAN \t 132\n",
      "UNITED STATES \t 133\n",
      "VIETNAM \t 137\n",
      "IRAQ \t 150\n",
      "KENYA \t 120\n",
      "BRAZIL \t 154\n",
      "ALGERIA \t 122\n",
      "CHINA \t 197\n",
      "input the Brand(like \"Brand#13\" Press Enter to quite):\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 6 here\n",
    "# 对输入的品牌，输出一个关于国家的table\n",
    "# \n",
    "part_raw=spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/part.tbl\",sep=\"|\",inferSchema=True)\n",
    "part = part_raw.toDF(\"PartKey\",\"Name\",\"Mfgr\",\"Brand\",\"Type\",\"Size\",\"Container\",\n",
    "                     \"RetailPrice\",\"Comment\",\"extra\").drop(\"extra\").cache()\n",
    "part.createOrReplaceTempView(\"part\")\n",
    "\n",
    "partsupp_raw=spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/partsupp.tbl\",sep=\"|\",inferSchema=True)\n",
    "partsupp = partsupp_raw.toDF(\"PartKey\",\"SuppKey\",\"Availqty\",\"SupplyCost\",\"Comment\",\"extra\").drop(\"extra\").cache()\n",
    "partsupp.createOrReplaceTempView(\"partsupp\")\n",
    "\n",
    "while True:\n",
    "    brand=input(\"input the Brand(like \\\"Brand#13\\\" Press Enter to quite):\")\n",
    "    if len(brand)==0:\n",
    "        break\n",
    "        \n",
    "    country_count_table=part.select(\"Brand\",\"PartKey\")\\\n",
    "            .filter(part.Brand==brand)\\\n",
    "            .join(partsupp,\"PartKey\")\\\n",
    "            .select(\"SuppKey\")\\\n",
    "            .join(supplier,\"SuppKey\")\\\n",
    "            .select(\"NationKey\")\\\n",
    "            .groupBy(\"NationKey\")\\\n",
    "            .count()\\\n",
    "            .join(nation,\"NationKey\")\\\n",
    "            .select(\"Name\",\"count\").collect()\n",
    "            \n",
    "    print(\"Country \\t Count\")\n",
    "    for i in country_count_table:\n",
    "        print(i[0],'\\t',i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 7 (5/25 marks)\n",
    "In the cell below, write code that will prompt for a Nation name.   Report, for each year, the total number of orders placed by customers from the specified Nation.\n",
    "\n",
    "You may answer this question with or without using `sql` - whichever you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input the Nation name(Press Enter to quite):INDIA\n",
      "the total number of orders placed by customers from the INDIA for each year\n",
      "year \t number of orders\n",
      "1992 \t 848\n",
      "1993 \t 850\n",
      "1994 \t 913\n",
      "1995 \t 888\n",
      "1996 \t 796\n",
      "1997 \t 880\n",
      "1998 \t 552\n",
      "input the Nation name(Press Enter to quite):CHINA\n",
      "the total number of orders placed by customers from the CHINA for each year\n",
      "year \t number of orders\n",
      "1992 \t 917\n",
      "1993 \t 902\n",
      "1994 \t 994\n",
      "1995 \t 884\n",
      "1996 \t 948\n",
      "1997 \t 915\n",
      "1998 \t 581\n",
      "input the Nation name(Press Enter to quite):\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 7 here\n",
    "\n",
    "while True:\n",
    "    nation_name=input(\"input the Nation name(Press Enter to quite):\")\n",
    "    if len(nation_name)==0:\n",
    "        break\n",
    "    raw=spark.sql(\"SELECT from_unixtime(unix_timestamp(OrderDate),\\\"Y\\\") as year,CustKey from orders\\\n",
    "                                 WHERE orders.CustKey\\\n",
    "                                 in (SELECT CustKey FROM customer WHERE customer.NationKey \\\n",
    "                                 in (SELECT NationKey FROM nation WHERE nation.Name=\\\"{0}\\\"))\".format(nation_name))\n",
    "    num_orders_country=sorted(raw.groupBy('year').agg({'CustKey': 'count'}).collect())\n",
    "    print(\"the total number of orders placed by customers from the\",nation_name,\"for each year\")\n",
    "    print(\"year \\t number of orders\")\n",
    "    for i in num_orders_country:\n",
    "        print(i[0],'\\t',i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Don't forget to save your workbook!   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
